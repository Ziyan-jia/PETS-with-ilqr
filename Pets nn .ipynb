{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pets nn .ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOhRn+1yMOPaKpnemTct0l+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"hZJECYec7i8l","executionInfo":{"status":"ok","timestamp":1644362619250,"user_tz":300,"elapsed":641,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}}},"outputs":[],"source":["%matplotlib notebook\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import math\n","import random\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import pandas as pd\n","from mpi4py import MPI \n","comm = MPI.COMM_WORLD\n","size = comm.Get_size()\n","rank = comm.Get_rank()\n","boss = rank==0\n","# Seed RNG\n","np.random.seed(rank)\n","# python -m pip install mpi4py"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yLggIEFE7mMx","executionInfo":{"status":"ok","timestamp":1644362644164,"user_tz":300,"elapsed":20104,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}},"outputId":"7846ca93-b9af-41bd-868f-8e47a31da62a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["training_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ziyan/ILQR/training data.csv')\n","test_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ziyan/ILQR/test data.csv')\n","valid_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ziyan/ILQR/valid data.csv')\n","training_data = np.array(training_data)\n","test_data = np.array(test_data)\n","valid_data = np.array(valid_data)"],"metadata":{"id":"FJoVss_Z7oxm","executionInfo":{"status":"ok","timestamp":1644362645262,"user_tz":300,"elapsed":1105,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Model():\n","  def __init__(self, input_dim, output_dim, learning_rate, seed):\n","\n","    torch.manual_seed(seed)\n","\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim\n","    self.hidden_units = 100\n","        # Instantiate model\n","    self.model = torch.nn.Sequential(\n","    torch.nn.Linear(self.input_dim, self.hidden_units, bias=True),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(self.hidden_units, self.hidden_units, bias=True),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(self.hidden_units, self.hidden_units, bias=True),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(self.hidden_units, self.hidden_units, bias=True),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(self.hidden_units, self.output_dim, bias=True)\n","    ).to(torch.device('cpu'))\n","    # Instantiate optimizer\n","    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n","\n","  def adjust_learning_rate(self):\n","    for param_group in self.optimizer.param_groups:\n","      param_group['lr'] = param_group['lr']*0.999099\n","\n","  def softplus(self, x):\n","    \"\"\" Compute softplus \"\"\"\n","    softplus = torch.log(1+torch.exp(x))\n","    # Avoid infinities due to taking the exponent\n","    softplus = torch.where(softplus==float('inf'), x, softplus)\n","    return softplus\n","\n","  def NLL(self, mean, var, truth):\n","    diff = torch.sub(truth, mean)\n","    var = self.softplus(var)\n","      # Compute loss \n","    loss = torch.mean(torch.div(diff**2, 2*var))\n","    loss += torch.mean(0.5*torch.log(var))\n","    return loss.sum()\n","\n","  def forward(self, inputs, input_type):\n","    \"\"\" Forward pass for a given input\n","        :input:     (state,action)-pair\n","        :returns:   means and variances given input\n","    \"\"\"\n","    # Compute output of model\n","    if input_type == \"nparray\":            \n","      x = torch.from_numpy(inputs).float()\n","      x = x.view(-1,self.input_dim)\n","      out = self.model(x)\n","      mean, var = torch.split(out, self.output_dim//2, dim=1)\n","      var = self.softplus(var)\n","      return mean.detach().numpy(), var.detach().numpy()\n","    else:\n","      inputs = inputs.view(-1,self.input_dim)\n","      out = self.model(inputs)\n","      mean, var = torch.split(out, self.output_dim//2, dim=1)\n","      var = self.softplus(var)\n","      return mean, var\n","      \n","  def step(self, inputs, true_out):\n","    \"\"\" Execute gradient step given the samples in the minibatch \"\"\"\n","    # Convert input and true_out to useable tensors\n","    x = torch.from_numpy(inputs).float()\n","    y = torch.from_numpy(true_out).float()\n","      \n","\n","    # Compute output of model\n","    out = self.model(x)\n","    mean, var = torch.split(out, self.output_dim//2, dim=1)\n","    \n","\n","\n","    # Compute loss \n","    self.nll = self.NLL(mean, var, y)\n","\n","    # Backpropagate the loss\n","    self.optimizer.zero_grad()\n","    self.nll.backward()\n","    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10)\n","    self.optimizer.step()\n","\n","    self.adjust_learning_rate()\n","\n","  def compute_errors(self, train_in, validation_in, test_in):\n","    \"\"\" Compute loss on the training, validation and test data \"\"\"\n","    # Training data\n","    train_in = torch.from_numpy(train_in).float()\n","    train_in, train_out = torch.split(train_in, [21,14], dim=1)\n","    mean, var = self.forward(train_in, \"tensor\")\n","    train_loss = self.NLL(mean, var, train_out).item()\n","\n","    # Validation data\n","    validation_in = torch.from_numpy(validation_in).float()\n","    validation_in, val_out = torch.split(validation_in, [21,14], dim=1)\n","    mean, var = self.forward(validation_in, \"tensor\")\n","    val_loss = self.NLL(mean, var, val_out).item()\n","\n","    # Test data\n","    test_in = torch.from_numpy(test_in).float()\n","    test_in, test_out = torch.split(test_in, [21,14], dim=1)\n","    mean, var = self.forward(test_in, \"tensor\")\n","    test_loss = self.NLL(mean, var, test_out).item()\n","\n","    return train_loss, val_loss, test_loss"],"metadata":{"id":"GrwpZQiC7rYp","executionInfo":{"status":"ok","timestamp":1644366451723,"user_tz":300,"elapsed":334,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["ensemble_size = 4          # Ensemble size per core\n","epochs = 20000\n","learning_rate = 0.000012\n","training_samples = 1000\n","validation_samples = training_samples\n","test_samples = 1000\n","batch_size = 16\n","measurements = epochs//200  # Measure every n steps\n","\n","# Define network architecture\n","input_dim = 21\n","output_dim = 28\n","    # Define systems\n","    # current available data sets: SimpleTwoDimensional(N, seed)\n","datamic_systems = [\"linear\"]\n","\n","seeds = np.random.randint(1e4, size=ensemble_size)\n","    # Allocate for losses\n","train_error = np.zeros((ensemble_size, measurements))\n","validation_error = np.zeros(train_error.shape)\n","test_error = np.zeros(train_error.shape)\n","    \n","for system in datamic_systems:\n","        # Instantiate class objects\n","        # data = datamics.datamics(training_samples, system)\n","        # Initialize models\n","  ensemble = [Model(input_dim, output_dim, learning_rate, seeds[i]) for i in range(ensemble_size)]\n","      # Initialize & allocate\n","  ensemble_mean = np.zeros((test_samples, 14))\n","  ensemble_var = np.zeros((test_samples, 14))\n","\n","      \n","\n","  # Train an ensemble of probabilistic networks:\n","  i = 0\n","  for model in ensemble:\n","    j = 0\n","    for epoch in range(epochs):\n","      indices = np.random.choice(range(training_samples-1), size=batch_size,replace=False)\n","      minibatch=training_data[indices]\n","      model_in = minibatch[:,:21]\n","      y = minibatch[:,21:]\n","      model.step(model_in, y)\n","      if (epoch+1)%(epochs//measurements)==0:\n","        # Compute error on train, test and validation sets\n","        train_error[i,j], validation_error[i,j], test_error[i,j] = model.compute_errors(training_data, valid_data, test_data)\n","        j += 1\n","      if (epoch+1)%100 == 0:\n","        print('Epoch %s, Train loss %s, Valid loss %s, Test loss %s'%(epoch, train_error[i,j-1],validation_error[i,j-1], test_error[i,j-1]))\n","\n","      # Test model on training samples\n","\n","  #mean, var = model.forward(test_data[:,:21], \"nparray\")\n","\n","\n","      # Add to ensemble mean and variance\n","  #ensemble_mean += mean \n","  #ensemble_var += var + mean**2\n","\n","  i += 1\n","    \n","\n","\n","        \n","   \n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"R7yu4ktrDwJl","executionInfo":{"status":"error","timestamp":1644371687303,"user_tz":300,"elapsed":64852,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}},"outputId":"f5f44658-4d04-4bab-c2e4-e626e7c28bab"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 99, Train loss 0.0, Valid loss 0.0, Test loss 0.0\n","Epoch 199, Train loss 304.93646240234375, Valid loss 308.46697998046875, Test loss 314.2660217285156\n","Epoch 299, Train loss 304.93646240234375, Valid loss 308.46697998046875, Test loss 314.2660217285156\n","Epoch 399, Train loss 180.85240173339844, Valid loss 182.2140350341797, Test loss 187.1732635498047\n","Epoch 499, Train loss 180.85240173339844, Valid loss 182.2140350341797, Test loss 187.1732635498047\n","Epoch 599, Train loss 112.6772232055664, Valid loss 113.50374603271484, Test loss 116.99335479736328\n","Epoch 699, Train loss 112.6772232055664, Valid loss 113.50374603271484, Test loss 116.99335479736328\n","Epoch 799, Train loss 77.87274932861328, Valid loss 78.4239273071289, Test loss 80.82411193847656\n","Epoch 899, Train loss 77.87274932861328, Valid loss 78.4239273071289, Test loss 80.82411193847656\n","Epoch 999, Train loss 58.916873931884766, Valid loss 59.322853088378906, Test loss 61.018409729003906\n","Epoch 1099, Train loss 58.916873931884766, Valid loss 59.322853088378906, Test loss 61.018409729003906\n","Epoch 1199, Train loss 47.14330291748047, Valid loss 47.44703674316406, Test loss 48.73796463012695\n","Epoch 1299, Train loss 47.14330291748047, Valid loss 47.44703674316406, Test loss 48.73796463012695\n","Epoch 1399, Train loss 39.52117156982422, Valid loss 39.750972747802734, Test loss 40.80413818359375\n","Epoch 1499, Train loss 39.52117156982422, Valid loss 39.750972747802734, Test loss 40.80413818359375\n","Epoch 1599, Train loss 34.29372024536133, Valid loss 34.480926513671875, Test loss 35.37074279785156\n","Epoch 1699, Train loss 34.29372024536133, Valid loss 34.480926513671875, Test loss 35.37074279785156\n","Epoch 1799, Train loss 30.54410171508789, Valid loss 30.699941635131836, Test loss 31.4779109954834\n","Epoch 1899, Train loss 30.54410171508789, Valid loss 30.699941635131836, Test loss 31.4779109954834\n","Epoch 1999, Train loss 27.806724548339844, Valid loss 27.943265914916992, Test loss 28.641185760498047\n","Epoch 2099, Train loss 27.806724548339844, Valid loss 27.943265914916992, Test loss 28.641185760498047\n","Epoch 2199, Train loss 25.771015167236328, Valid loss 25.892131805419922, Test loss 26.53140640258789\n","Epoch 2299, Train loss 25.771015167236328, Valid loss 25.892131805419922, Test loss 26.53140640258789\n","Epoch 2399, Train loss 24.207277297973633, Valid loss 24.31857681274414, Test loss 24.911109924316406\n","Epoch 2499, Train loss 24.207277297973633, Valid loss 24.31857681274414, Test loss 24.911109924316406\n","Epoch 2599, Train loss 22.998239517211914, Valid loss 23.10236930847168, Test loss 23.65949058532715\n","Epoch 2699, Train loss 22.998239517211914, Valid loss 23.10236930847168, Test loss 23.65949058532715\n","Epoch 2799, Train loss 22.049314498901367, Valid loss 22.147939682006836, Test loss 22.677448272705078\n","Epoch 2899, Train loss 22.049314498901367, Valid loss 22.147939682006836, Test loss 22.677448272705078\n","Epoch 2999, Train loss 21.294422149658203, Valid loss 21.388168334960938, Test loss 21.89646339416504\n","Epoch 3099, Train loss 21.294422149658203, Valid loss 21.388168334960938, Test loss 21.89646339416504\n","Epoch 3199, Train loss 20.693313598632812, Valid loss 20.7830810546875, Test loss 21.27407455444336\n","Epoch 3299, Train loss 20.693313598632812, Valid loss 20.7830810546875, Test loss 21.27407455444336\n","Epoch 3399, Train loss 20.206439971923828, Valid loss 20.293216705322266, Test loss 20.77016258239746\n","Epoch 3499, Train loss 20.206439971923828, Valid loss 20.293216705322266, Test loss 20.77016258239746\n","Epoch 3599, Train loss 19.811492919921875, Valid loss 19.89588737487793, Test loss 20.36163902282715\n","Epoch 3699, Train loss 19.811492919921875, Valid loss 19.89588737487793, Test loss 20.36163902282715\n","Epoch 3799, Train loss 19.491262435913086, Valid loss 19.574146270751953, Test loss 20.030393600463867\n","Epoch 3899, Train loss 19.491262435913086, Valid loss 19.574146270751953, Test loss 20.030393600463867\n","Epoch 3999, Train loss 19.2277889251709, Valid loss 19.30942153930664, Test loss 19.757795333862305\n","Epoch 4099, Train loss 19.2277889251709, Valid loss 19.30942153930664, Test loss 19.757795333862305\n","Epoch 4199, Train loss 19.013011932373047, Valid loss 19.093387603759766, Test loss 19.5358943939209\n","Epoch 4299, Train loss 19.013011932373047, Valid loss 19.093387603759766, Test loss 19.5358943939209\n","Epoch 4399, Train loss 18.834861755371094, Valid loss 18.914154052734375, Test loss 19.351566314697266\n","Epoch 4499, Train loss 18.834861755371094, Valid loss 18.914154052734375, Test loss 19.351566314697266\n","Epoch 4599, Train loss 18.68760108947754, Valid loss 18.76597023010254, Test loss 19.19923210144043\n","Epoch 4699, Train loss 18.68760108947754, Valid loss 18.76597023010254, Test loss 19.19923210144043\n","Epoch 4799, Train loss 18.56614875793457, Valid loss 18.64386558532715, Test loss 19.073604583740234\n","Epoch 4899, Train loss 18.56614875793457, Valid loss 18.64386558532715, Test loss 19.073604583740234\n","Epoch 4999, Train loss 18.465728759765625, Valid loss 18.542926788330078, Test loss 18.969690322875977\n","Epoch 5099, Train loss 18.465728759765625, Valid loss 18.542926788330078, Test loss 18.969690322875977\n","Epoch 5199, Train loss 18.38227653503418, Valid loss 18.45904541015625, Test loss 18.883331298828125\n","Epoch 5299, Train loss 18.38227653503418, Valid loss 18.45904541015625, Test loss 18.883331298828125\n","Epoch 5399, Train loss 18.312829971313477, Valid loss 18.38919448852539, Test loss 18.811548233032227\n","Epoch 5499, Train loss 18.312829971313477, Valid loss 18.38919448852539, Test loss 18.811548233032227\n","Epoch 5599, Train loss 18.255138397216797, Valid loss 18.331214904785156, Test loss 18.751873016357422\n","Epoch 5699, Train loss 18.255138397216797, Valid loss 18.331214904785156, Test loss 18.751873016357422\n","Epoch 5799, Train loss 18.207244873046875, Valid loss 18.28304672241211, Test loss 18.702360153198242\n","Epoch 5899, Train loss 18.207244873046875, Valid loss 18.28304672241211, Test loss 18.702360153198242\n","Epoch 5999, Train loss 18.16716194152832, Valid loss 18.242748260498047, Test loss 18.660890579223633\n","Epoch 6099, Train loss 18.16716194152832, Valid loss 18.242748260498047, Test loss 18.660890579223633\n","Epoch 6199, Train loss 18.133853912353516, Valid loss 18.20928955078125, Test loss 18.62643051147461\n","Epoch 6299, Train loss 18.133853912353516, Valid loss 18.20928955078125, Test loss 18.62643051147461\n","Epoch 6399, Train loss 18.1062068939209, Valid loss 18.181501388549805, Test loss 18.59783172607422\n","Epoch 6499, Train loss 18.1062068939209, Valid loss 18.181501388549805, Test loss 18.59783172607422\n","Epoch 6599, Train loss 18.083009719848633, Valid loss 18.158172607421875, Test loss 18.573820114135742\n","Epoch 6699, Train loss 18.083009719848633, Valid loss 18.158172607421875, Test loss 18.573820114135742\n","Epoch 6799, Train loss 18.063669204711914, Valid loss 18.138731002807617, Test loss 18.553810119628906\n","Epoch 6899, Train loss 18.063669204711914, Valid loss 18.138731002807617, Test loss 18.553810119628906\n","Epoch 6999, Train loss 18.04766845703125, Valid loss 18.12264633178711, Test loss 18.537260055541992\n","Epoch 7099, Train loss 18.04766845703125, Valid loss 18.12264633178711, Test loss 18.537260055541992\n","Epoch 7199, Train loss 18.03417205810547, Valid loss 18.109067916870117, Test loss 18.523296356201172\n","Epoch 7299, Train loss 18.03417205810547, Valid loss 18.109067916870117, Test loss 18.523296356201172\n","Epoch 7399, Train loss 18.02315330505371, Valid loss 18.097986221313477, Test loss 18.511892318725586\n","Epoch 7499, Train loss 18.02315330505371, Valid loss 18.097986221313477, Test loss 18.511892318725586\n","Epoch 7599, Train loss 18.01345443725586, Valid loss 18.08823585510254, Test loss 18.501861572265625\n","Epoch 7699, Train loss 18.01345443725586, Valid loss 18.08823585510254, Test loss 18.501861572265625\n","Epoch 7799, Train loss 18.005638122558594, Valid loss 18.08038330078125, Test loss 18.493772506713867\n","Epoch 7899, Train loss 18.005638122558594, Valid loss 18.08038330078125, Test loss 18.493772506713867\n","Epoch 7999, Train loss 17.99957847595215, Valid loss 18.074296951293945, Test loss 18.487504959106445\n","Epoch 8099, Train loss 17.99957847595215, Valid loss 18.074296951293945, Test loss 18.487504959106445\n","Epoch 8199, Train loss 17.99427032470703, Valid loss 18.06896209716797, Test loss 18.482009887695312\n","Epoch 8299, Train loss 17.99427032470703, Valid loss 18.06896209716797, Test loss 18.482009887695312\n","Epoch 8399, Train loss 17.9893798828125, Valid loss 18.064044952392578, Test loss 18.476945877075195\n","Epoch 8499, Train loss 17.9893798828125, Valid loss 18.064044952392578, Test loss 18.476945877075195\n","Epoch 8599, Train loss 17.985212326049805, Valid loss 18.05985450744629, Test loss 18.472639083862305\n","Epoch 8699, Train loss 17.985212326049805, Valid loss 18.05985450744629, Test loss 18.472639083862305\n","Epoch 8799, Train loss 17.981637954711914, Valid loss 18.05626106262207, Test loss 18.468944549560547\n","Epoch 8899, Train loss 17.981637954711914, Valid loss 18.05626106262207, Test loss 18.468944549560547\n","Epoch 8999, Train loss 17.97906494140625, Valid loss 18.053674697875977, Test loss 18.46628189086914\n","Epoch 9099, Train loss 17.97906494140625, Valid loss 18.053674697875977, Test loss 18.46628189086914\n","Epoch 9199, Train loss 17.977542877197266, Valid loss 18.052143096923828, Test loss 18.46470832824707\n","Epoch 9299, Train loss 17.977542877197266, Valid loss 18.052143096923828, Test loss 18.46470832824707\n","Epoch 9399, Train loss 17.97624969482422, Valid loss 18.050846099853516, Test loss 18.463369369506836\n","Epoch 9499, Train loss 17.97624969482422, Valid loss 18.050846099853516, Test loss 18.463369369506836\n","Epoch 9599, Train loss 17.975122451782227, Valid loss 18.04970932006836, Test loss 18.462200164794922\n","Epoch 9699, Train loss 17.975122451782227, Valid loss 18.04970932006836, Test loss 18.462200164794922\n","Epoch 9799, Train loss 17.97443962097168, Valid loss 18.04902458190918, Test loss 18.461498260498047\n","Epoch 9899, Train loss 17.97443962097168, Valid loss 18.04902458190918, Test loss 18.461498260498047\n","Epoch 9999, Train loss 17.97408103942871, Valid loss 18.048662185668945, Test loss 18.46112632751465\n","Epoch 10099, Train loss 17.97408103942871, Valid loss 18.048662185668945, Test loss 18.46112632751465\n","Epoch 10199, Train loss 17.97377586364746, Valid loss 18.048358917236328, Test loss 18.4608097076416\n","Epoch 10299, Train loss 17.97377586364746, Valid loss 18.048358917236328, Test loss 18.4608097076416\n","Epoch 10399, Train loss 17.9735164642334, Valid loss 18.048095703125, Test loss 18.460542678833008\n","Epoch 10499, Train loss 17.9735164642334, Valid loss 18.048095703125, Test loss 18.460542678833008\n","Epoch 10599, Train loss 17.97337532043457, Valid loss 18.047954559326172, Test loss 18.46039390563965\n","Epoch 10699, Train loss 17.97337532043457, Valid loss 18.047954559326172, Test loss 18.46039390563965\n","Epoch 10799, Train loss 17.973295211791992, Valid loss 18.04787254333496, Test loss 18.460309982299805\n","Epoch 10899, Train loss 17.973295211791992, Valid loss 18.04787254333496, Test loss 18.460309982299805\n","Epoch 10999, Train loss 17.973224639892578, Valid loss 18.047800064086914, Test loss 18.460235595703125\n","Epoch 11099, Train loss 17.973224639892578, Valid loss 18.047800064086914, Test loss 18.460235595703125\n","Epoch 11199, Train loss 17.97316551208496, Valid loss 18.04774284362793, Test loss 18.46017837524414\n","Epoch 11299, Train loss 17.97316551208496, Valid loss 18.04774284362793, Test loss 18.46017837524414\n","Epoch 11399, Train loss 17.973134994506836, Valid loss 18.047710418701172, Test loss 18.46014404296875\n","Epoch 11499, Train loss 17.973134994506836, Valid loss 18.047710418701172, Test loss 18.46014404296875\n","Epoch 11599, Train loss 17.973115921020508, Valid loss 18.047693252563477, Test loss 18.460124969482422\n","Epoch 11699, Train loss 17.973115921020508, Valid loss 18.047693252563477, Test loss 18.460124969482422\n","Epoch 11799, Train loss 17.973098754882812, Valid loss 18.04767417907715, Test loss 18.460105895996094\n","Epoch 11899, Train loss 17.973098754882812, Valid loss 18.04767417907715, Test loss 18.460105895996094\n","Epoch 11999, Train loss 17.97308349609375, Valid loss 18.04766082763672, Test loss 18.460092544555664\n","Epoch 12099, Train loss 17.97308349609375, Valid loss 18.04766082763672, Test loss 18.460092544555664\n","Epoch 12199, Train loss 17.97307777404785, Valid loss 18.04765510559082, Test loss 18.4600887298584\n","Epoch 12299, Train loss 17.97307777404785, Valid loss 18.04765510559082, Test loss 18.4600887298584\n","Epoch 12399, Train loss 17.973072052001953, Valid loss 18.047649383544922, Test loss 18.460081100463867\n","Epoch 12499, Train loss 17.973072052001953, Valid loss 18.047649383544922, Test loss 18.460081100463867\n","Epoch 12599, Train loss 17.973068237304688, Valid loss 18.047643661499023, Test loss 18.4600772857666\n","Epoch 12699, Train loss 17.973068237304688, Valid loss 18.047643661499023, Test loss 18.4600772857666\n","Epoch 12799, Train loss 17.973064422607422, Valid loss 18.04764175415039, Test loss 18.46007537841797\n","Epoch 12899, Train loss 17.973064422607422, Valid loss 18.04764175415039, Test loss 18.46007537841797\n","Epoch 12999, Train loss 17.973064422607422, Valid loss 18.04764175415039, Test loss 18.460073471069336\n","Epoch 13099, Train loss 17.973064422607422, Valid loss 18.04764175415039, Test loss 18.460073471069336\n","Epoch 13199, Train loss 17.97306251525879, Valid loss 18.047639846801758, Test loss 18.460073471069336\n","Epoch 13299, Train loss 17.97306251525879, Valid loss 18.047639846801758, Test loss 18.460073471069336\n","Epoch 13399, Train loss 17.97306251525879, Valid loss 18.047637939453125, Test loss 18.460071563720703\n","Epoch 13499, Train loss 17.97306251525879, Valid loss 18.047637939453125, Test loss 18.460071563720703\n","Epoch 13599, Train loss 17.97306251525879, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 13699, Train loss 17.97306251525879, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 13799, Train loss 17.97306251525879, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 13899, Train loss 17.97306251525879, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 13999, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14099, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14199, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14299, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14399, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14499, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14599, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14699, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14799, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14899, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 14999, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15099, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15199, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15299, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15399, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15499, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15599, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15699, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15799, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15899, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 15999, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16099, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16199, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16299, Train loss 17.973060607910156, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16399, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16499, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16599, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16699, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16799, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16899, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 16999, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17099, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17199, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17299, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17399, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17499, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17599, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17699, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17799, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17899, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 17999, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18099, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18199, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18299, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18399, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18499, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18599, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18699, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18799, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18899, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 18999, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19099, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19199, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19299, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19399, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19499, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19599, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19699, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19799, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19899, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 19999, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 99, Train loss 17.973058700561523, Valid loss 18.047636032104492, Test loss 18.46006965637207\n","Epoch 199, Train loss 260.5921936035156, Valid loss 261.6175537109375, Test loss 265.0007629394531\n","Epoch 299, Train loss 260.5921936035156, Valid loss 261.6175537109375, Test loss 265.0007629394531\n","Epoch 399, Train loss 135.10870361328125, Valid loss 134.9894561767578, Test loss 137.6624298095703\n","Epoch 499, Train loss 135.10870361328125, Valid loss 134.9894561767578, Test loss 137.6624298095703\n","Epoch 599, Train loss 82.3880615234375, Valid loss 82.30291748046875, Test loss 84.08685302734375\n","Epoch 699, Train loss 82.3880615234375, Valid loss 82.30291748046875, Test loss 84.08685302734375\n","Epoch 799, Train loss 56.71675491333008, Valid loss 56.694114685058594, Test loss 57.875152587890625\n","Epoch 899, Train loss 56.71675491333008, Valid loss 56.694114685058594, Test loss 57.875152587890625\n","Epoch 999, Train loss 42.57949447631836, Valid loss 42.56438446044922, Test loss 43.444881439208984\n","Epoch 1099, Train loss 42.57949447631836, Valid loss 42.56438446044922, Test loss 43.444881439208984\n","Epoch 1199, Train loss 34.04419708251953, Valid loss 34.03782272338867, Test loss 34.74087905883789\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-f91b27093e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mmodel_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mmeasurements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Compute error on train, test and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-22bf5b09b44f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, inputs, true_out)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m         \"\"\"\n\u001b[0;32m-> 1520\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1544\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[0;32m-> 1546\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1491\u001b[0m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1689\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["def simulate_system(x, u):\n","  ensemble_mean = np.zeros((1,14))\n","  ensemble_mean = torch.from_numpy(ensemble_mean)\n","  ensemble_var = np.zeros((1,14))\n","  ensemble_var = torch.from_numpy(ensemble_var)\n","  x = x.T\n","  u = u.T\n","  x = np.hstack((x,u))\n","  x = x.astype(np.float32)\n","  #x = torch.from_numpy(x)\n","  for model in ensemble:\n","    mean, var = model.forward(x, \"nparray\")\n","    ensemble_mean += mean\n","    ensemble_var += var + mean ** 2\n","    print(mean)\n","    \n","  #gathered_means = comm.gather(ensemble_mean)\n","  #gathered_vars = comm.gather(ensemble_var)\n","  #stacked_means = np.stack(gathered_means, axis=0)\n","  #stacked_vars = np.stack(gathered_vars, axis=0)\n","  #assert(stacked_means.shape == stacked_vars.shape)\n","  #ensemble_mean = stacked_means / ensemble_size\n","  #ensemble_var = stacked_vars / ensemble_size\n","  #ensemble_var = ensemble_var - ensemble_mean**2\n","  ensemble_mean = ensemble_mean / ensemble_size\n","  ensemble_var = ensemble_var / ensemble_size\n","  ensemble_var = ensemble_var - ensemble_mean**2\n","\n","  #ensemble_mean=ensemble_mean.detach().numpy()\n","  #ensemble_var=ensemble_var.detach().numpy()\n","  #ensemble_var = ensemble_var.reshape((14,1))\n","    #for i in range(14):\n","        #if ensemble_var[i]<=0:\n","            #ensemble_var[i] = 0.0001\n","        #if ensemble_var[i] != float:\n","            #ensemble_var[i] = 10000\n","  return ensemble_mean,ensemble_var"],"metadata":{"id":"6D88i6xdWY-E","executionInfo":{"status":"ok","timestamp":1644370207101,"user_tz":300,"elapsed":150,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["ZZ = np.array(training_data[1,:14])\n","UU = np.array(training_data[1,14:21])\n","simulate_system(ZZ, UU)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hfi0DAlXqza","executionInfo":{"status":"ok","timestamp":1644370291786,"user_tz":300,"elapsed":142,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}},"outputId":"0899e6d0-ce51-4e6e-ed5e-aeb9853a4cf8"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-1.1337963  -1.2965276   0.8364221   1.1230072   0.7402898  -0.76636934\n","   0.04833825  4.1550684   4.049585    5.3085546   0.2869519  -3.808255\n","  -3.0492458  14.856499  ]]\n","[[ 3.4293373   4.028005    1.3922765   0.27590925 -0.24528256 -0.2998793\n","   0.41948146  3.3247755   4.1542635  -2.7659194   2.6210191  -9.371803\n","  -2.609622   -3.349002  ]]\n","[[-0.33334512 -0.5841472   0.19395602  2.168904    0.30806386  0.32784948\n","  -0.05072077  1.63087    -3.4722543  -1.9151219  -0.12373321 -9.704905\n","  -1.3379205   1.8444126 ]]\n","[[  0.20540015  -0.10438987   0.4650936   -0.80695325   0.17842488\n","    0.34767938  -0.15283595   7.8216257    0.80170804  -5.7370768\n","   -6.8539004   -3.2436802  -19.528582    10.439626  ]]\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.5419,  0.5107,  0.7219,  0.6902,  0.2454, -0.0977,  0.0661,  4.2331,\n","           1.3833, -1.2774, -1.0174, -6.5322, -6.6313,  5.9479]],\n","        dtype=torch.float64),\n"," tensor([[ 7.0661,  8.3468,  1.9046,  3.9016,  3.0293,  1.3511,  4.8771, 20.4033,\n","          22.3667, 36.0519, 27.8954, 42.5745, 82.6791, 91.6404]],\n","        dtype=torch.float64))"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["print(training_data[1,21:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMAMtYrJbX5Y","executionInfo":{"status":"ok","timestamp":1644370293340,"user_tz":300,"elapsed":157,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}},"outputId":"724b0a86-8e31-4c7f-f6f4-38e409dbf118"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[-6.5633120e-01 -3.3784660e-01  7.0910764e-01  3.7938920e-01\n"," -1.1324311e-01  2.3932531e-02 -1.6888889e+00  6.0335493e+00\n"," -3.7846596e+00  2.3132986e+01  5.7166986e+00  2.8675690e+01\n"," -5.8162304e+01 -1.0000000e+02]\n"]}]},{"cell_type":"code","source":["ZZ = np.array(training_data[0,:14])\n","UU = np.array(training_data[0,14:21])\n","simulate_system(ZZ, UU)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eVaF9Lttj5kn","executionInfo":{"status":"ok","timestamp":1644294124355,"user_tz":300,"elapsed":354,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}},"outputId":"b3179095-b25b-455f-c1ad-af99faea9f05"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.48502004  0.8605413  -1.7585945   0.6993234   0.55698407  0.23327248\n","  -0.9355381  -4.790979    2.683198    3.048272    2.359308   -3.0158556\n","   7.388413   -3.2617886 ]]\n","[[ 0.25046027 -1.5253627  -1.5284578   0.8861569  -0.11947447  1.487737\n","  -1.3665055  -1.9053917   0.37650782 -0.90301704 -0.2531714   3.581868\n","  -6.3261127  -2.6135273 ]]\n","[[-0.2069063  -1.0923793  -0.98949605 -0.70573354  1.0254303  -0.45563734\n","  -0.33450377 -2.0688815   1.1788868  -2.5026858   0.05464757  4.252819\n","  -1.353702    1.3997132 ]]\n","[[  0.05011452   1.7463226    0.54926676  -0.30345455   0.31023115\n","   -0.5115269   -0.01689413  -1.648943    -0.04219985 -12.346002\n","    1.1288185   -9.253822     2.6356936    3.1973126 ]]\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([[-9.7838e-02, -2.7195e-03, -9.3182e-01,  1.4407e-01,  4.4329e-01,\n","           1.8846e-01, -6.6336e-01, -2.6035e+00,  1.0491e+00, -3.1759e+00,\n","           8.2240e-01, -1.1087e+00,  5.8607e-01, -3.1957e-01]],\n","        dtype=torch.float64),\n"," tensor([[ 2.4228,  4.3733,  5.3372,  2.6494,  3.5205,  3.5906,  3.0167, 17.7586,\n","          13.3182, 47.5430, 19.4407, 56.3754, 48.6377, 40.2676]],\n","        dtype=torch.float64))"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["# Gather data from all cores\n","gathered_means = comm.gather(ensemble_mean)\n","gathered_vars = comm.gather(ensemble_var)\n","gathered_train_err = comm.gather(train_error)\n","gathered_val_err = comm.gather(validation_error)\n","gathered_test_err = comm.gather(test_error)\n","# Compute ensemble mean and averages\n","stacked_means = np.stack(gathered_means, axis=0)\n","\n","stacked_vars = np.stack(gathered_vars, axis=0)\n","assert(stacked_means.shape == stacked_vars.shape)\n","ensemble_mean = stacked_means / ensemble_size\n","ensemble_var = stacked_vars / ensemble_size\n","ensemble_var = ensemble_var - ensemble_mean**2\n","print(ensemble_mean)\n","\n","# Save ensemble\n","#np.save(\"data/ensemble_mean_%s\"%(system), ensemble_mean)\n","#np.save(\"data/ensemble_var_%s\"%(system), ensemble_var)\n","# Compute mean errors\n","stacked_train_err = np.stack(gathered_train_err, axis=0)\n","mean_train_err = np.mean(np.mean(gathered_train_err, axis=0), axis=0)\n","#np.save(\"data/train_error_%s\"%(system), mean_train_err)\n","stacked_val_err = np.stack(gathered_val_err, axis=0)\n","mean_val_err = np.mean(np.mean(gathered_val_err, axis=0), axis=0)\n","#np.save(\"data/val_error_%s\"%(system), mean_val_err)\n","stacked_test_err = np.stack(gathered_test_err, axis=0)\n","mean_test_err = np.mean(np.mean(gathered_test_err, axis=0), axis=0)\n","#np.save(\"data/test_error_%s\"%(system), mean_test_err)\n","\n","\n","# Save things for plotting\n","#np.save(\"data/test_samples\", data.test_data)\n","#print(\"\\n...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKw04F98ZL2K","executionInfo":{"status":"ok","timestamp":1644290339603,"user_tz":300,"elapsed":325,"user":{"displayName":"Ziyan Jia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17425287401465399422"}},"outputId":"eaedd4b1-e617-45e4-87fe-7096218d3291"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[ 2.14819041 -0.62289528 -0.2587198  ... -2.04153483 -2.85288429\n","   -4.69466043]\n","  [ 3.16661927 -0.16044442 -0.31342744 ... -4.06609368 -2.9272452\n","   -1.51780862]\n","  [ 1.75710507 -0.48084529  0.17528443 ... -2.75613517 -2.20122683\n","   -0.20081882]\n","  ...\n","  [ 1.68377027 -0.84343077  0.10162868 ... -3.37603571 -3.5194281\n","   -0.73134935]\n","  [ 2.62247431 -0.0666911  -1.13640465 ... -3.08553943 -3.25247735\n","   -1.29030269]\n","  [ 2.36711986 -0.5144839   0.20863129 ... -3.22959562 -2.44806156\n","   -0.5163473 ]]]\n"]}]}]}